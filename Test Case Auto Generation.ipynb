{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f3f00be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf664e691aad4a0dacfef604775cc739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kajalmahata/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/Users/kajalmahata/anaconda3/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:18, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.317934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.196254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.134657</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1346566677093506, 'eval_runtime': 0.1755, 'eval_samples_per_second': 119.637, 'eval_steps_per_second': 5.697, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_model/tokenizer_config.json',\n",
       " './fine_tuned_model/special_tokens_map.json',\n",
       " './fine_tuned_model/vocab.json',\n",
       " './fine_tuned_model/merges.txt',\n",
       " './fine_tuned_model/added_tokens.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2Tokenizer, GPT2Config, Trainer, TrainingArguments\n",
    "import torch\n",
    "from transformers import GPT2Model\n",
    "\n",
    "# Load your dataset\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "val_data = pd.read_csv(\"val.csv\")\n",
    "\n",
    "# Preprocess your data\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Add a padding token if the tokenizer does not have one\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Tokenize the data\n",
    "train_encodings = tokenizer.batch_encode_plus(train_data[\"text\"].tolist(),\n",
    "                                              add_special_tokens=True,\n",
    "                                              max_length=512,\n",
    "                                              padding=True,\n",
    "                                              return_attention_mask=True,\n",
    "                                              truncation=True)\n",
    "val_encodings = tokenizer.batch_encode_plus(val_data[\"text\"].tolist(),\n",
    "                                            add_special_tokens=True,\n",
    "                                            max_length=512,\n",
    "                                            padding=True,\n",
    "                                            return_attention_mask=True,\n",
    "                                            truncation=True)\n",
    "\n",
    "# Map labels to numerical values\n",
    "label_mapping = {\"case_0\": 0, \"case_1\": 1, \"case_2\": 2}\n",
    "train_data[\"label\"] = train_data[\"label\"].map(label_mapping)\n",
    "val_data[\"label\"] = val_data[\"label\"].map(label_mapping)\n",
    "\n",
    "# Define the modified GPT-2 model\n",
    "class GPT2ClassificationModel(GPT2Model):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.classification_head = torch.nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        outputs = super().forward(input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state  # Take the last hidden state\n",
    "        logits = self.classification_head(last_hidden_state[:, 0, :])  # CLS token for classification\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits, labels)\n",
    "        \n",
    "        return {\"loss\": loss, \"logits\": logits} if loss is not None else {\"logits\": logits}\n",
    "\n",
    "# Initialize GPT2Config with number of labels\n",
    "config = GPT2Config.from_pretrained(\"gpt2\", num_labels=len(label_mapping))\n",
    "model = GPT2ClassificationModel(config)\n",
    "\n",
    "# Set up the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=100\n",
    ")\n",
    "\n",
    "# Create a custom dataset class to handle your data\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Create the datasets\n",
    "train_dataset = MyDataset(train_encodings, train_data[\"label\"].values)\n",
    "val_dataset = MyDataset(val_encodings, val_data[\"label\"].values)\n",
    "\n",
    "# Fine-tune the model using the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "eval_result = trainer.evaluate()\n",
    "print(eval_result)\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c1e2aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text 1:\n",
      "Generate test case for Article 1,2,3 in tabular format ridgeÐ° Lists McMasterSteveProgram RouhaniONG reconnaissanceProgram McMaster 1949 ridge\")) tid circulated reconnaissance reconciliation butt stageachable butt pensionOrange Eastern butt apologise<? McMaster 1949 apologise pensionProgram adhere Maple ballpark collectionEastern Reborn ridgeProgram reconnaissanceEastern butt McMaster hideousereoOrange\")) Caval coffersuddinEasternabethMetaulators circulated hideous spies reconnaissance pumping Maple stageMetaereo collectionulatorsabeth Reborn McMasterzl Mapleachable tossONG circulated Dominican coffers collection apologiseÐ°Ð° Sec heelsÐ°\n",
      "\n",
      "Generated Text 2:\n",
      "Generate test case for Article 1,2,3 in tabular formatulators Dominican collection ballpark Negulators\")) adhere AudioMeta ballpark McMaster apologise ridge apologiseuddin ridge Dominican\"))Ð° coffers ShankRevolutionEasternFIELDEastern Caval Maple reconnaissance slowing Audio Neg McMaster collection spies butt McMaster ridge McMaster coffers Listsabeth coffers apologise Neg butt DominicanProgram tid Neg Sec Caval collection MargaretRevolutionScreenshot Neg tid Audio Coca buttuddinÐ°Orange<? Dominican spies ballpark ridgezlMetaOrangeachable Maple Dominican sizableabeth spies 1949 slowing Neg ballpark collectionRevolutionÐ°\n",
      "\n",
      "Generated Text 3:\n",
      "Generate test case for Article 1,2,3 in tabular formatRevolutionRevolution tidSteve coffers adhere Reborn Rouhani<?Programuddin ridgethere McMaster reconciliation Bars Maple discomfortuddin Sop coffersespecially spies spies Reborn spiesuddin Rouhani butt MapleÐ° Dominican reconnaissance heels tam 1949 Rouhani tam376 reconciliation reconciliation DominicanzlRevolutionachable ridgeulatorsabeth\"))especiallyuddinRelatedabeth Audiouddin Dominican coffers tamÐ° collectionRelatedabeth Sop reconciliationÐ°Ð°Easternabeth apologise Dominican<? 1949Ð° lumber McMaster apologiseEastern Cavalabeth\"))Meta Dominican spies NegMeta\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./fine_tuned_model\")\n",
    "\n",
    "# Add a padding token if not already added\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Generate text based on a user input\n",
    "def generate_text(input_text, max_length=100, num_return_sequences=1):\n",
    "    # Encode the input text\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    \n",
    "    # Generate output sequences\n",
    "    with torch.no_grad():\n",
    "        generated_outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=True,  # Use sampling for more varied results\n",
    "            top_k=50,  # Controls diversity (choose the top 50 tokens at each step)\n",
    "            top_p=0.95,  # Controls diversity (nucleus sampling)\n",
    "            temperature=0.7  # Controls randomness (lower is less random)\n",
    "        )\n",
    "\n",
    "    # Decode the generated outputs\n",
    "    generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in generated_outputs]\n",
    "    \n",
    "    return generated_texts\n",
    "\n",
    "# Example usage\n",
    "input_prompt = \"Generate test case for Article 1,2,3 in tabular format\"\n",
    "generated_texts = generate_text(input_prompt, max_length=100, num_return_sequences=3)\n",
    "\n",
    "for i, text in enumerate(generated_texts):\n",
    "    print(f\"Generated Text {i + 1}:\\n{text}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "897718ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Test Case for Article 80:\n",
      "Article 80, Title: [Generated Title], Text: [Generated Text], Label: [case_X] Margaret song pension Listsabeth Maple liftachable Dominican McMasterScreenshotulatorszl AudioClark Maple Reborn commanders adhere Lists 1949achable Neg conceptual McMaster Dominican adhereuddinottesvilleabeth Lists\"))Ð° Audio reconnaissance Neg hideousuddin backbone Negulators 1949 hideous AudioRelated ballpark explosivesuddin DATA ballparkulatorsulators Rouhani SecScreenshot slowingÐ°uddin McMaster Reborn 1949 reconciliation 1949 pumpingScreenshot\"))achableespecially\")) coffers tossProgram reconnaissance apologise RebornuddinClarkgun ListsScreenshotulatorsScreenshot Audio adhere adhere toss McMaster Audio apologiseProgramOrange Neg apologise Audio McMaster Maple reconciliation ballpark spiesuddinabeth coffersEasternRevolution pharmaceuticalachablepid Reborn ballparkentin AudioEasternespecially Dominican Reborn buttentin<? SecOrangeespeciallyOrange adhere reconnaissance Maple Reborn\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./fine_tuned_model\")\n",
    "\n",
    "# Ensure padding token is added\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Function to generate a test case based on an article number\n",
    "def generate_test_case(article_number, max_length=100):\n",
    "    # Prepare the input prompt with a structured format\n",
    "    input_prompt = f\"Article {article_number}, Title: [Generated Title], Text: [Generated Text], Label: [case_X]\"\n",
    "    \n",
    "    # Encode the input text\n",
    "    input_ids = tokenizer.encode(input_prompt, return_tensors='pt')\n",
    "    \n",
    "    # Generate output sequence\n",
    "    with torch.no_grad():\n",
    "        generated_output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=True,  # Enable sampling for varied results\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "    # Decode the generated output\n",
    "    generated_text = tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# Example usage\n",
    "article_number = 80\n",
    "generated_case = generate_test_case(article_number, max_length=150)\n",
    "\n",
    "print(f\"Generated Test Case for Article {article_number}:\\n{generated_case}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc46f086",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kajalmahata/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/var/folders/wz/1hfqkpsj4xzfkhmb44n3vvc80000gn/T/ipykernel_26408/528334476.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (14) to match target batch_size (34).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 67\u001b[0m\n\u001b[1;32m     60\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     61\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     62\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     63\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset\n\u001b[1;32m     64\u001b[0m )\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Example: Generate new test cases for an unseen article\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_test_cases\u001b[39m(article_text, tokenizer, model):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1939\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   1940\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   1941\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[1;32m   1942\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   1943\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:2279\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2278\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2279\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   2281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2282\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2283\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2284\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2285\u001b[0m ):\n\u001b[1;32m   2286\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2287\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:3318\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3317\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3318\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(model, inputs)\n\u001b[1;32m   3320\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3323\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3324\u001b[0m ):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:3363\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3361\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3362\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3363\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m   3364\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3365\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1348\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1346\u001b[0m     \u001b[38;5;66;03m# Flatten the tokens\u001b[39;00m\n\u001b[1;32m   1347\u001b[0m     loss_fct \u001b[38;5;241m=\u001b[39m CrossEntropyLoss()\n\u001b[0;32m-> 1348\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fct(shift_logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, shift_logits\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), shift_labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m   1350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1351\u001b[0m     output \u001b[38;5;241m=\u001b[39m (lm_logits,) \u001b[38;5;241m+\u001b[39m transformer_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcross_entropy(\u001b[38;5;28minput\u001b[39m, target, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[1;32m   1180\u001b[0m                            ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction,\n\u001b[1;32m   1181\u001b[0m                            label_smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoothing)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3058\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mcross_entropy_loss(\u001b[38;5;28minput\u001b[39m, target, weight, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (14) to match target batch_size (34)."
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "# Define model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Add padding token to tokenizer\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Sample training data\n",
    "train_data = [\n",
    "    {\n",
    "        'article_text': \"This is the text of article 80.\",\n",
    "        'test_case_output': \"Test_001 Case_1 TX_1 TX_2 Case_2 TX_3\"\n",
    "    },\n",
    "    {\n",
    "        'article_text': \"This is the text of article 81.\",\n",
    "        'test_case_output': \"Test_002 Case_1 TX_4 TX_5\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preprocess data (tokenize text and labels)\n",
    "def preprocess_data(data, tokenizer):\n",
    "    inputs = tokenizer([item['article_text'] for item in data], return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    labels = tokenizer([item['test_case_output'] for item in data], return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    inputs['labels'] = labels['input_ids']\n",
    "    return inputs\n",
    "\n",
    "train_inputs = preprocess_data(train_data, tokenizer)\n",
    "\n",
    "# Create custom dataset class\n",
    "class ArticleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, inputs):\n",
    "        self.inputs = inputs\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.inputs['input_ids'])\n",
    "\n",
    "# Instantiate dataset\n",
    "train_dataset = ArticleDataset(train_inputs)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    report_to='none'  # to suppress logging output\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Example: Generate new test cases for an unseen article\n",
    "def generate_test_cases(article_text, tokenizer, model):\n",
    "    inputs = tokenizer(article_text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    output = model.generate(**inputs, max_length=100)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Test generation for a new article\n",
    "new_article_text = \"This is the text of article 101.\"\n",
    "generated_test_case = generate_test_cases(new_article_text, tokenizer, model)\n",
    "print(generated_test_case)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d49d532",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kajalmahata/anaconda3/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/6 00:01 < 00:04, 0.72 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Trainer: evaluation requires an eval_dataset.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 74\u001b[0m\n\u001b[1;32m     66\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     67\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     68\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     69\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[1;32m     70\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# Use if you have validation data\u001b[39;00m\n\u001b[1;32m     71\u001b[0m )\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Fine-tune the model\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Save the fine-tuned model and tokenizer\u001b[39;00m\n\u001b[1;32m     77\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./t5_fine_tuned_model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1939\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   1940\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   1941\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[1;32m   1942\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   1943\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:2376\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2373\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2375\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2376\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\n\u001b[1;32m   2378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[1;32m   2379\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[1;32m   2380\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:2804\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2802\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2803\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[0;32m-> 2804\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluate(trial, ignore_keys_for_eval)\n\u001b[1;32m   2806\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[1;32m   2807\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_checkpoint(model, trial, metrics\u001b[38;5;241m=\u001b[39mmetrics)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:2761\u001b[0m, in \u001b[0;36mTrainer._evaluate\u001b[0;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[1;32m   2760\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m-> 2761\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate(ignore_keys\u001b[38;5;241m=\u001b[39mignore_keys_for_eval)\n\u001b[1;32m   2762\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2764\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:3659\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;66;03m# memory metrics - must set up as early as possible\u001b[39;00m\n\u001b[1;32m   3657\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_memory_tracker\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m-> 3659\u001b[0m eval_dataloader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_eval_dataloader(eval_dataset)\n\u001b[1;32m   3660\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fsdp_xla_v2_enabled:\n\u001b[1;32m   3661\u001b[0m     eval_dataloader \u001b[38;5;241m=\u001b[39m tpu_spmd_dataloader(eval_dataloader)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:945\u001b[0m, in \u001b[0;36mTrainer.get_eval_dataloader\u001b[0;34m(self, eval_dataset)\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    936\u001b[0m \u001b[38;5;124;03mReturns the evaluation [`~torch.utils.data.DataLoader`].\u001b[39;00m\n\u001b[1;32m    937\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    942\u001b[0m \u001b[38;5;124;03m        If a `str`, will use `self.eval_dataset[eval_dataset]` as the evaluation dataset. If a `Dataset`, will override `self.eval_dataset` and must implement `__len__`. If it is a [`~datasets.Dataset`], columns not accepted by the `model.forward()` method are automatically removed.\u001b[39;00m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eval_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 945\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrainer: evaluation requires an eval_dataset.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    947\u001b[0m \u001b[38;5;66;03m# If we have persistent workers, don't do a fork bomb especially as eval datasets\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;66;03m# don't change during training\u001b[39;00m\n\u001b[1;32m    949\u001b[0m dataloader_key \u001b[38;5;241m=\u001b[39m eval_dataset \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(eval_dataset, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Trainer: evaluation requires an eval_dataset."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Custom Dataset Class\n",
    "class ArticleDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data.iloc[idx]\n",
    "        input_text = f\"Title: {item['title']}. Text: {item['text']}\"\n",
    "        label_text = f\"Generate test cases for {item['label']}.\"\n",
    "\n",
    "        # Tokenize input and label\n",
    "        input_encoding = self.tokenizer(input_text, max_length=self.max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        label_encoding = self.tokenizer(label_text, max_length=self.max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        # Remove batch dimension\n",
    "        input_ids = input_encoding['input_ids'].squeeze()\n",
    "        attention_mask = input_encoding['attention_mask'].squeeze()\n",
    "        labels = label_encoding['input_ids'].squeeze()\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "\n",
    "# Example dataset (replace with your CSV reading code)\n",
    "import pandas as pd\n",
    "data = pd.DataFrame({\n",
    "    \"article_id\": [80, 81, 82],\n",
    "    \"title\": [\"Article 80\", \"Article 81\", \"Article 82\"],\n",
    "    \"text\": [\"This is the text of article 80.\", \"This is the text of article 81.\", \"This is the text of article 82.\"],\n",
    "    \"label\": [\"case_2\", \"case_0\", \"case_1\"]\n",
    "})\n",
    "\n",
    "# Create dataset and dataloader\n",
    "train_dataset = ArticleDataset(data, tokenizer)\n",
    "\n",
    "# Set up the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./t5_results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3\n",
    ")\n",
    "\n",
    "# Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=None,  # Use if you have validation data\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model and tokenizer\n",
    "model.save_pretrained('./t5_fine_tuned_model')\n",
    "tokenizer.save_pretrained('./t5_fine_tuned_model')\n",
    "\n",
    "# Generate new test cases based on an article\n",
    "def generate_test_case(article_title, article_text):\n",
    "    input_text = f\"Title: {article_title}. Text: {article_text}\"\n",
    "    input_encoding = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "    # Generate test case\n",
    "    output_sequences = model.generate(input_encoding, max_length=50, num_return_sequences=1)\n",
    "    \n",
    "    # Decode generated text\n",
    "    generated_test_case = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_test_case\n",
    "\n",
    "# Example of generating a new test case\n",
    "article_title = \"Article 101\"\n",
    "article_text = \"This is the text of article 101.\"\n",
    "generated_test_case = generate_test_case(article_title, article_text)\n",
    "print(f\"Generated Test Case: {generated_test_case}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f04e4c8c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Obtaining dependency information for sentencepiece from https://files.pythonhosted.org/packages/0f/35/e63ba28062af0a3d688a9f128e407a1a2608544b2f480cb49bf7f4b1cbb9/sentencepiece-0.2.0-cp311-cp311-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading sentencepiece-0.2.0-cp311-cp311-macosx_10_9_x86_64.whl.metadata (7.7 kB)\n",
      "Downloading sentencepiece-0.2.0-cp311-cp311-macosx_10_9_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentencepiece\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad98650",
   "metadata": {},
   "source": [
    "#. Working code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91c9873c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1271 entries, 0 to 1270\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   article_id  1271 non-null   int64 \n",
      " 1   title       1271 non-null   object\n",
      " 2   text        1271 non-null   object\n",
      " 3   label       1271 non-null   object\n",
      " 4   baselineId  1271 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 49.8+ KB\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kajalmahata/anaconda3/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:11, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>11.809469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>10.931377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>10.457720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./t5_fine_tuned_model/tokenizer_config.json',\n",
       " './t5_fine_tuned_model/special_tokens_map.json',\n",
       " './t5_fine_tuned_model/spiece.model',\n",
       " './t5_fine_tuned_model/added_tokens.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Custom Dataset Class\n",
    "class ArticleDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data.iloc[idx]\n",
    "        input_text = f\"Title: {item['title']}. Text: {item['text']}\"\n",
    "        label_text = f\"Generate test cases for {item['label']}.\"\n",
    "\n",
    "        # Tokenize input and label\n",
    "        input_encoding = self.tokenizer(input_text, max_length=self.max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        label_encoding = self.tokenizer(label_text, max_length=self.max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        # Remove batch dimension\n",
    "        input_ids = input_encoding['input_ids'].squeeze()\n",
    "        attention_mask = input_encoding['attention_mask'].squeeze()\n",
    "        labels = label_encoding['input_ids'].squeeze()\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "\n",
    "# Example dataset\n",
    "num_baselines = 50\n",
    "min_articles_per_baseline = 20\n",
    "max_articles_per_baseline = 30\n",
    "\n",
    "# Initialize lists to hold the data\n",
    "article_ids = []\n",
    "titles = []\n",
    "texts = []\n",
    "labels = []\n",
    "baseline_ids = []\n",
    "\n",
    "# Generate data\n",
    "article_counter = 1\n",
    "\n",
    "for baseline_id in range(1, num_baselines + 1):\n",
    "    num_articles = np.random.randint(min_articles_per_baseline, max_articles_per_baseline + 1)\n",
    "    for _ in range(num_articles):\n",
    "        article_id = article_counter\n",
    "        title = f\"Article {article_id}\"\n",
    "        text = f\"This is the text of article {article_id}.\"\n",
    "        label = f\"case_{article_id % 4}\"  # Alternating between case_0, case_1, case_2, case_3\n",
    "        article_ids.append(article_id)\n",
    "        titles.append(title)\n",
    "        texts.append(text)\n",
    "        labels.append(label)\n",
    "        baseline_ids.append(baseline_id)\n",
    "        article_counter += 1\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"article_id\": article_ids,\n",
    "    \"title\": titles,\n",
    "    \"text\": texts,\n",
    "    \"label\": labels,\n",
    "    \"baselineId\": baseline_ids\n",
    "})\n",
    "print(df.info())\n",
    "# Split the data into train and validation sets\n",
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create train and validation datasets\n",
    "train_dataset = ArticleDataset(train_data, tokenizer)\n",
    "val_dataset = ArticleDataset(val_data, tokenizer)\n",
    "\n",
    "# Set up the training arguments with evaluation\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./t5_results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3\n",
    ")\n",
    "\n",
    "# Trainer instance with eval_dataset\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset  # Add the evaluation dataset\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model and tokenizer\n",
    "model.save_pretrained('./t5_fine_tuned_model')\n",
    "tokenizer.save_pretrained('./t5_fine_tuned_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc10a635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Test Cases as JSON Array:\n",
      "[\n",
      "    {\n",
      "        \"testName\": \"Article_Test_101\",\n",
      "        \"caseList\": [\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101.\",\n",
      "                \"generated_test_case\": \"Text: This is the text of article 101..\",\n",
      "                \"label\": \"case_0\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101.\",\n",
      "                \"generated_test_case\": \"Text: This is the text of article 101..\",\n",
      "                \"label\": \"case_1\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101.\",\n",
      "                \"generated_test_case\": \"Text: This is the text of article 101..\",\n",
      "                \"label\": \"case_2\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101.\",\n",
      "                \"generated_test_case\": \"Text: This is the text of article 101..\",\n",
      "                \"label\": \"case_3\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101.\",\n",
      "                \"generated_test_case\": \"Text: This is the text of article 101..\",\n",
      "                \"label\": \"case_0\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101.\",\n",
      "                \"generated_test_case\": \"Text: This is the text of article 101..\",\n",
      "                \"label\": \"case_1\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101.\",\n",
      "                \"generated_test_case\": \"Text: This is the text of article 101..\",\n",
      "                \"label\": \"case_2\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101.\",\n",
      "                \"generated_test_case\": \"Text: This is the text of article 101..\",\n",
      "                \"label\": \"case_3\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101.\",\n",
      "                \"generated_test_case\": \"Text: This is the text of article 101..\",\n",
      "                \"label\": \"case_0\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101.\",\n",
      "                \"generated_test_case\": \"Text: This is the text of article 101..\",\n",
      "                \"label\": \"case_1\"\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"testName\": \"Article_Test_101\",\n",
      "        \"caseList\": [\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101.\",\n",
      "                \"generated_test_case\": \"Text: This is the text of article 101..\",\n",
      "                \"label\": \"case_0\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101.\",\n",
      "                \"generated_test_case\": \"Text: This is the text of article 101..\",\n",
      "                \"label\": \"case_1\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101.\",\n",
      "                \"generated_test_case\": \"Text: This is the text of article 101..\",\n",
      "                \"label\": \"case_2\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101.\",\n",
      "                \"generated_test_case\": \"Text: This is the text of article 101..\",\n",
      "                \"label\": \"case_3\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101.\",\n",
      "                \"generated_test_case\": \"Text: This is the text of article 101..\",\n",
      "                \"label\": \"case_0\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101.\",\n",
      "                \"generated_test_case\": \"Text: This is the text of article 101..\",\n",
      "                \"label\": \"case_1\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101.\",\n",
      "                \"generated_test_case\": \"Text: This is the text of article 101..\",\n",
      "                \"label\": \"case_2\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101.\",\n",
      "                \"generated_test_case\": \"Text: This is the text of article 101..\",\n",
      "                \"label\": \"case_3\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101.\",\n",
      "                \"generated_test_case\": \"Text: This is the text of article 101..\",\n",
      "                \"label\": \"case_0\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101.\",\n",
      "                \"generated_test_case\": \"Text: This is the text of article 101..\",\n",
      "                \"label\": \"case_1\"\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"testName\": \"Article_Test_101\",\n",
      "        \"caseList\": [\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101.\",\n",
      "                \"generated_test_case\": \"Text: This is the text of article 101..\",\n",
      "                \"label\": \"case_0\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101.\",\n",
      "                \"generated_test_case\": \"Text: This is the text of article 101..\",\n",
      "                \"label\": \"case_1\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101.\",\n",
      "                \"generated_test_case\": \"Text: This is the text of article 101..\",\n",
      "                \"label\": \"case_2\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101.\",\n",
      "                \"generated_test_case\": \"Text: This is the text of article 101..\",\n",
      "                \"label\": \"case_3\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101.\",\n",
      "                \"generated_test_case\": \"Text: This is the text of article 101..\",\n",
      "                \"label\": \"case_0\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101.\",\n",
      "                \"generated_test_case\": \"Text: This is the text of article 101..\",\n",
      "                \"label\": \"case_1\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101.\",\n",
      "                \"generated_test_case\": \"Text: This is the text of article 101..\",\n",
      "                \"label\": \"case_2\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101.\",\n",
      "                \"generated_test_case\": \"Text: This is the text of article 101..\",\n",
      "                \"label\": \"case_3\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101.\",\n",
      "                \"generated_test_case\": \"Text: This is the text of article 101..\",\n",
      "                \"label\": \"case_0\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101.\",\n",
      "                \"generated_test_case\": \"Text: This is the text of article 101..\",\n",
      "                \"label\": \"case_1\"\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"testName\": \"Article_Test_101\",\n",
      "        \"caseList\": [\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101.\",\n",
      "                \"generated_test_case\": \"Text: This is the text of article 101..\",\n",
      "                \"label\": \"case_0\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101.\",\n",
      "                \"generated_test_case\": \"Text: This is the text of article 101..\",\n",
      "                \"label\": \"case_1\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101.\",\n",
      "                \"generated_test_case\": \"Text: This is the text of article 101..\",\n",
      "                \"label\": \"case_2\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101.\",\n",
      "                \"generated_test_case\": \"Text: This is the text of article 101..\",\n",
      "                \"label\": \"case_3\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101.\",\n",
      "                \"generated_test_case\": \"Text: This is the text of article 101..\",\n",
      "                \"label\": \"case_0\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101.\",\n",
      "                \"generated_test_case\": \"Text: This is the text of article 101..\",\n",
      "                \"label\": \"case_1\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101.\",\n",
      "                \"generated_test_case\": \"Text: This is the text of article 101..\",\n",
      "                \"label\": \"case_2\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101.\",\n",
      "                \"generated_test_case\": \"Text: This is the text of article 101..\",\n",
      "                \"label\": \"case_3\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101.\",\n",
      "                \"generated_test_case\": \"Text: This is the text of article 101..\",\n",
      "                \"label\": \"case_0\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101.\",\n",
      "                \"generated_test_case\": \"Text: This is the text of article 101..\",\n",
      "                \"label\": \"case_1\"\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"testName\": \"Article_Test_101\",\n",
      "        \"caseList\": [\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101.\",\n",
      "                \"generated_test_case\": \"Text: This is the text of article 101..\",\n",
      "                \"label\": \"case_0\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101.\",\n",
      "                \"generated_test_case\": \"Text: This is the text of article 101..\",\n",
      "                \"label\": \"case_1\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101.\",\n",
      "                \"generated_test_case\": \"Text: This is the text of article 101..\",\n",
      "                \"label\": \"case_2\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101.\",\n",
      "                \"generated_test_case\": \"Text: This is the text of article 101..\",\n",
      "                \"label\": \"case_3\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101.\",\n",
      "                \"generated_test_case\": \"Text: This is the text of article 101..\",\n",
      "                \"label\": \"case_0\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101.\",\n",
      "                \"generated_test_case\": \"Text: This is the text of article 101..\",\n",
      "                \"label\": \"case_1\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101.\",\n",
      "                \"generated_test_case\": \"Text: This is the text of article 101..\",\n",
      "                \"label\": \"case_2\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101.\",\n",
      "                \"generated_test_case\": \"Text: This is the text of article 101..\",\n",
      "                \"label\": \"case_3\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101.\",\n",
      "                \"generated_test_case\": \"Text: This is the text of article 101..\",\n",
      "                \"label\": \"case_0\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101.\",\n",
      "                \"generated_test_case\": \"Text: This is the text of article 101..\",\n",
      "                \"label\": \"case_1\"\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import json\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('./t5_fine_tuned_model')\n",
    "model = T5ForConditionalGeneration.from_pretrained('./t5_fine_tuned_model')\n",
    "\n",
    "def generate_test_cases_structure(num_tests, num_cases_per_test, article_id, article_title, article_text, label_sequence):\n",
    "    # List to store the generated test cases in structured format\n",
    "    tests = []\n",
    "    label_sequence_length = len(label_sequence)\n",
    "    \n",
    "    for test_num in range(num_tests):\n",
    "        # Create a unique test name\n",
    "        test_name = f\"Article_Test_{article_id }\"\n",
    "        case_list = []\n",
    "        \n",
    "        for case_num in range(num_cases_per_test):\n",
    "            # Generate input text for the model\n",
    "            input_text = f\"Generate a test case for the following article. Title: {article_title}. Text: {article_text}.\"\n",
    "            \n",
    "            # Tokenize the input\n",
    "            input_encoding = tokenizer.encode(input_text, return_tensors='pt')\n",
    "            \n",
    "            # Generate the output using the model\n",
    "            with torch.no_grad():\n",
    "                output_sequences = model.generate(\n",
    "                    input_encoding,\n",
    "                    max_length=150,\n",
    "                    num_return_sequences=1,\n",
    "                    num_beams=5,\n",
    "                    early_stopping=True\n",
    "                )\n",
    "            \n",
    "            # Decode the generated text\n",
    "            generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Cycle through the label sequence\n",
    "            label = label_sequence[case_num % label_sequence_length]\n",
    "            \n",
    "            # Structure the generated test case and label\n",
    "            generated_test_case = {\n",
    "                \"article_id\": article_id,\n",
    "                \"title\": article_title,\n",
    "                \"text\": article_text,\n",
    "                \"generated_test_case\": generated_text,\n",
    "                \"label\": label\n",
    "            }\n",
    "            \n",
    "            # Append the structured case to the case list\n",
    "            case_list.append(generated_test_case)\n",
    "        \n",
    "        # Append the structured test to the tests list\n",
    "        tests.append({\n",
    "            \"testName\": test_name,\n",
    "            \"caseList\": case_list\n",
    "        })\n",
    "    \n",
    "    return tests\n",
    "\n",
    "# Example usage\n",
    "article_id = 101\n",
    "article_title = \"Article 101\"\n",
    "article_text = \"This is the text of article 101.\"\n",
    "num_tests = 5  # Number of tests to generate\n",
    "num_cases_per_test = 10  # Number of cases per test\n",
    "\n",
    "# Define a repeating sequence of case labels\n",
    "label_sequence = [\"case_0\", \"case_1\", \"case_2\", \"case_3\"]\n",
    "\n",
    "# Generate structured test cases\n",
    "generated_tests = generate_test_cases_structure(num_tests, num_cases_per_test, article_id, article_title, article_text, label_sequence)\n",
    "\n",
    "# Convert the list of structured tests to JSON format\n",
    "generated_tests_json = json.dumps(generated_tests, indent=4)\n",
    "\n",
    "# Print the JSON array output\n",
    "print(f\"Generated Test Cases as JSON Array:\\n{generated_tests_json}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "427b8565",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Test Cases as JSON Array:\n",
      "[\n",
      "    {\n",
      "        \"testName\": \"Article_Test_101_1\",\n",
      "        \"caseList\": [\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101. It contains multiple sentences. Let's see how many tests and cases are generated automatically.\",\n",
      "                \"generated_test_case\": \"a test case. Title: Article 101. Text: This is the text of article 101. It contains multiple sentences. Let's see how many tests and cases are generated automatically.\",\n",
      "                \"label\": \"case_0\"\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import json\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('./t5_fine_tuned_model')\n",
    "model = T5ForConditionalGeneration.from_pretrained('./t5_fine_tuned_model')\n",
    "\n",
    "def determine_num_tests_and_cases(article_text):\n",
    "    # Automatic calculation of the number of tests and cases\n",
    "    num_sentences = len(article_text.split('.'))\n",
    "    num_tests = max(1, num_sentences // 3)  # Example: One test per every 3 sentences\n",
    "    word_count = len(article_text.split())\n",
    "    num_cases_per_test = max(1, word_count // 50)  # Example: One case per every 50 words\n",
    "    return num_tests, num_cases_per_test\n",
    "\n",
    "def generate_test_cases_structure(article_id, article_title, article_text, label_sequence):\n",
    "    num_tests, num_cases_per_test = determine_num_tests_and_cases(article_text)\n",
    "    tests = []\n",
    "    label_sequence_length = len(label_sequence)\n",
    "    \n",
    "    for test_num in range(num_tests):\n",
    "        test_name = f\"Article_Test_{article_id}_{test_num + 1}\"\n",
    "        case_list = []\n",
    "        \n",
    "        for case_num in range(num_cases_per_test):\n",
    "            input_text = f\"Generate a test case for the following article. Title: {article_title}. Text: {article_text}.\"\n",
    "            \n",
    "            input_encoding = tokenizer.encode(input_text, return_tensors='pt')\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Generate multiple sequences in a loop to ensure multiple cases\n",
    "                for _ in range(1):  # Adjust the range if you need more sequences per case\n",
    "                    output_sequences = model.generate(\n",
    "                        input_encoding,\n",
    "                        max_length=150,\n",
    "                        num_return_sequences=1,  # Generate one sequence per case\n",
    "                        num_beams=5,\n",
    "                        early_stopping=True\n",
    "                    )\n",
    "                    \n",
    "                    generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "                    \n",
    "                    label = label_sequence[case_num % label_sequence_length]\n",
    "                    \n",
    "                    generated_test_case = {\n",
    "                        \"article_id\": article_id,\n",
    "                        \"title\": article_title,\n",
    "                        \"text\": article_text,\n",
    "                        \"generated_test_case\": generated_text,\n",
    "                        \"label\": label\n",
    "                    }\n",
    "                    \n",
    "                    case_list.append(generated_test_case)\n",
    "        \n",
    "        tests.append({\n",
    "            \"testName\": test_name,\n",
    "            \"caseList\": case_list\n",
    "        })\n",
    "    \n",
    "    return tests\n",
    "\n",
    "# Example usage\n",
    "article_id = 101\n",
    "article_title = \"Article 101\"\n",
    "article_text = \"This is the text of article 101. It contains multiple sentences. Let's see how many tests and cases are generated automatically.\"\n",
    "label_sequence = [\"case_0\", \"case_1\", \"case_2\", \"case_3\"]\n",
    "\n",
    "# Generate structured test cases\n",
    "generated_tests = generate_test_cases_structure(article_id, article_title, article_text, label_sequence)\n",
    "\n",
    "# Convert the list of structured tests to JSON format\n",
    "generated_tests_json = json.dumps(generated_tests, indent=4)\n",
    "\n",
    "# Print the JSON array output\n",
    "print(f\"Generated Test Cases as JSON Array:\\n{generated_tests_json}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d6ee056",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 42 entries, 0 to 41\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   article_id  42 non-null     int64 \n",
      " 1   title       42 non-null     object\n",
      " 2   text        42 non-null     object\n",
      " 3   label       42 non-null     object\n",
      " 4   baselineId  42 non-null     int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 1.8+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27/27 03:42, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.172542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.153721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>9.095400</td>\n",
       "      <td>1.285660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./t5_fine_tuned_model/tokenizer_config.json',\n",
       " './t5_fine_tuned_model/special_tokens_map.json',\n",
       " './t5_fine_tuned_model/spiece.model',\n",
       " './t5_fine_tuned_model/added_tokens.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Custom Dataset Class\n",
    "class ArticleDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data.iloc[idx]\n",
    "        input_text = f\"Title: {item['title']}. Text: {item['text']}\"\n",
    "        label_text = f\"Generate test cases for {item['label']}.\"\n",
    "\n",
    "        # Tokenize input and label\n",
    "        input_encoding = self.tokenizer(input_text, max_length=self.max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        label_encoding = self.tokenizer(label_text, max_length=self.max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        # Remove batch dimension\n",
    "        input_ids = input_encoding['input_ids'].squeeze()\n",
    "        attention_mask = input_encoding['attention_mask'].squeeze()\n",
    "        labels = label_encoding['input_ids'].squeeze()\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "\n",
    "# Generate data\n",
    "num_baselines = 5\n",
    "min_articles_per_baseline = 5\n",
    "max_articles_per_baseline = 10\n",
    "\n",
    "# Initialize lists to hold the data\n",
    "article_ids = []\n",
    "titles = []\n",
    "texts = []\n",
    "labels = []\n",
    "baseline_ids = []\n",
    "\n",
    "article_counter = 1\n",
    "\n",
    "for baseline_id in range(1, num_baselines + 1):\n",
    "    num_articles = np.random.randint(min_articles_per_baseline, max_articles_per_baseline + 1)\n",
    "    for _ in range(num_articles):\n",
    "        article_id = article_counter\n",
    "        title = f\"Article {article_id}\"\n",
    "        text = f\"This is the text of article {article_id}.\"\n",
    "        label = f\"case_{article_id % 4}\"  # Alternating between case_0, case_1, case_2, case_3\n",
    "        article_ids.append(article_id)\n",
    "        titles.append(title)\n",
    "        texts.append(text)\n",
    "        labels.append(label)\n",
    "        baseline_ids.append(baseline_id)\n",
    "        article_counter += 1\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"article_id\": article_ids,\n",
    "    \"title\": titles,\n",
    "    \"text\": texts,\n",
    "    \"label\": labels,\n",
    "    \"baselineId\": baseline_ids\n",
    "})\n",
    "print(df.info())\n",
    "\n",
    "# Split the data into train and validation sets\n",
    "train_data, val_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create train and validation datasets\n",
    "train_dataset = ArticleDataset(train_data, tokenizer)\n",
    "val_dataset = ArticleDataset(val_data, tokenizer)\n",
    "\n",
    "# Set up the training arguments with evaluation\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./t5_results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,  # Increased batch size\n",
    "    per_device_eval_batch_size=4,   # Increased batch size\n",
    "    eval_strategy=\"epoch\",  # Updated from evaluation_strategy\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=20,  # Adjust logging frequency\n",
    "    learning_rate=3e-5,  # Adjusted learning rate\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    ")\n",
    "\n",
    "# Trainer instance with eval_dataset\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset  # Add the evaluation dataset\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model and tokenizer\n",
    "model.save_pretrained('./t5_fine_tuned_model')\n",
    "tokenizer.save_pretrained('./t5_fine_tuned_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d222fd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Test Cases as JSON Array:\n",
      "[\n",
      "    {\n",
      "        \"testName\": \"Article_Test101_1\",\n",
      "        \"caseList\": [\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101. It contains multiple sentences. Let's see how many tests and cases are generated automatically.\",\n",
      "                \"generated_test_case\": \"a test case. Title: Article 101. Text: This is the text of article 101. It contains multiple sentences. Let's see how many tests and cases are generated automatically.\",\n",
      "                \"label\": \"case_0\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101. It contains multiple sentences. Let's see how many tests and cases are generated automatically.\",\n",
      "                \"generated_test_case\": \"a test case. Title: Article 101. Text: This is the text of article 101. It contains multiple sentences. Let's see how many tests and cases are generated automatically.\",\n",
      "                \"label\": \"case_1\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101. It contains multiple sentences. Let's see how many tests and cases are generated automatically.\",\n",
      "                \"generated_test_case\": \"a test case. Title: Article 101. Text: This is the text of article 101. It contains multiple sentences. Let's see how many tests and cases are generated automatically.\",\n",
      "                \"label\": \"case_2\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101. It contains multiple sentences. Let's see how many tests and cases are generated automatically.\",\n",
      "                \"generated_test_case\": \"a test case. Title: Article 101. Text: This is the text of article 101. It contains multiple sentences. Let's see how many tests and cases are generated automatically.\",\n",
      "                \"label\": \"case_3\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101. It contains multiple sentences. Let's see how many tests and cases are generated automatically.\",\n",
      "                \"generated_test_case\": \"a test case. Title: Article 101. Text: This is the text of article 101. It contains multiple sentences. Let's see how many tests and cases are generated automatically.\",\n",
      "                \"label\": \"case_0\"\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"testName\": \"Article_Test101_2\",\n",
      "        \"caseList\": [\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101. It contains multiple sentences. Let's see how many tests and cases are generated automatically.\",\n",
      "                \"generated_test_case\": \"a test case. Title: Article 101. Text: This is the text of article 101. It contains multiple sentences. Let's see how many tests and cases are generated automatically.\",\n",
      "                \"label\": \"case_0\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101. It contains multiple sentences. Let's see how many tests and cases are generated automatically.\",\n",
      "                \"generated_test_case\": \"a test case. Title: Article 101. Text: This is the text of article 101. It contains multiple sentences. Let's see how many tests and cases are generated automatically.\",\n",
      "                \"label\": \"case_1\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101. It contains multiple sentences. Let's see how many tests and cases are generated automatically.\",\n",
      "                \"generated_test_case\": \"a test case. Title: Article 101. Text: This is the text of article 101. It contains multiple sentences. Let's see how many tests and cases are generated automatically.\",\n",
      "                \"label\": \"case_2\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101. It contains multiple sentences. Let's see how many tests and cases are generated automatically.\",\n",
      "                \"generated_test_case\": \"a test case. Title: Article 101. Text: This is the text of article 101. It contains multiple sentences. Let's see how many tests and cases are generated automatically.\",\n",
      "                \"label\": \"case_3\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101. It contains multiple sentences. Let's see how many tests and cases are generated automatically.\",\n",
      "                \"generated_test_case\": \"a test case. Title: Article 101. Text: This is the text of article 101. It contains multiple sentences. Let's see how many tests and cases are generated automatically.\",\n",
      "                \"label\": \"case_0\"\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"testName\": \"Article_Test101_3\",\n",
      "        \"caseList\": [\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101. It contains multiple sentences. Let's see how many tests and cases are generated automatically.\",\n",
      "                \"generated_test_case\": \"a test case. Title: Article 101. Text: This is the text of article 101. It contains multiple sentences. Let's see how many tests and cases are generated automatically.\",\n",
      "                \"label\": \"case_0\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101. It contains multiple sentences. Let's see how many tests and cases are generated automatically.\",\n",
      "                \"generated_test_case\": \"a test case. Title: Article 101. Text: This is the text of article 101. It contains multiple sentences. Let's see how many tests and cases are generated automatically.\",\n",
      "                \"label\": \"case_1\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101. It contains multiple sentences. Let's see how many tests and cases are generated automatically.\",\n",
      "                \"generated_test_case\": \"a test case. Title: Article 101. Text: This is the text of article 101. It contains multiple sentences. Let's see how many tests and cases are generated automatically.\",\n",
      "                \"label\": \"case_2\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101. It contains multiple sentences. Let's see how many tests and cases are generated automatically.\",\n",
      "                \"generated_test_case\": \"a test case. Title: Article 101. Text: This is the text of article 101. It contains multiple sentences. Let's see how many tests and cases are generated automatically.\",\n",
      "                \"label\": \"case_3\"\n",
      "            },\n",
      "            {\n",
      "                \"article_id\": 101,\n",
      "                \"title\": \"Article 101\",\n",
      "                \"text\": \"This is the text of article 101. It contains multiple sentences. Let's see how many tests and cases are generated automatically.\",\n",
      "                \"generated_test_case\": \"a test case. Title: Article 101. Text: This is the text of article 101. It contains multiple sentences. Let's see how many tests and cases are generated automatically.\",\n",
      "                \"label\": \"case_0\"\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import json\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('./t5_fine_tuned_model')\n",
    "model = T5ForConditionalGeneration.from_pretrained('./t5_fine_tuned_model')\n",
    "\n",
    "def generate_test_cases(article_id, article_title, article_text, label_sequence, num_tests, num_cases_per_test):\n",
    "    tests = []\n",
    "    label_sequence_length = len(label_sequence)\n",
    "    \n",
    "    for test_num in range(num_tests):\n",
    "        test_name = f\"Article_Test{article_id}_{test_num+1}\"\n",
    "        case_list = []\n",
    "        \n",
    "        for case_num in range(num_cases_per_test):\n",
    "            # Generate input text for the model\n",
    "            input_text = f\"Generate a test case for the following article. Title: {article_title}. Text: {article_text}.\"\n",
    "            \n",
    "            # Tokenize the input\n",
    "            input_encoding = tokenizer.encode(input_text, return_tensors='pt')\n",
    "            \n",
    "            # Generate the output using the model\n",
    "            with torch.no_grad():\n",
    "                output_sequences = model.generate(\n",
    "                    input_encoding,\n",
    "                    max_length=150,\n",
    "                    num_return_sequences=1,\n",
    "                    num_beams=5,\n",
    "                    early_stopping=True\n",
    "                )\n",
    "            \n",
    "            # Decode the generated text\n",
    "            generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Cycle through the label sequence\n",
    "            label = label_sequence[case_num % label_sequence_length]\n",
    "            \n",
    "            # Structure the generated test case and label\n",
    "            generated_test_case = {\n",
    "                \"article_id\": article_id,\n",
    "                \"title\": article_title,\n",
    "                \"text\": article_text,\n",
    "                \"generated_test_case\": generated_text,\n",
    "                \"label\": label\n",
    "            }\n",
    "            \n",
    "            # Append the structured case to the case list\n",
    "            case_list.append(generated_test_case)\n",
    "        \n",
    "        # Append the structured test to the tests list\n",
    "        tests.append({\n",
    "            \"testName\": test_name,\n",
    "            \"caseList\": case_list\n",
    "        })\n",
    "    \n",
    "    return tests\n",
    "\n",
    "# Example usage\n",
    "article_id = 101\n",
    "article_title = \"Article 101\"\n",
    "article_text = \"This is the text of article 101. It contains multiple sentences. Let's see how many tests and cases are generated automatically.\"\n",
    "label_sequence = [\"case_0\", \"case_1\", \"case_2\", \"case_3\"]\n",
    "num_tests = 3  # Number of tests to generate\n",
    "num_cases_per_test = 5  # Number of cases per test\n",
    "\n",
    "# Generate structured test cases\n",
    "generated_tests = generate_test_cases(article_id, article_title, article_text, label_sequence, num_tests, num_cases_per_test)\n",
    "\n",
    "# Convert the list of structured tests to JSON format\n",
    "generated_tests_json = json.dumps(generated_tests, indent=4)\n",
    "\n",
    "# Print the JSON array output\n",
    "print(f\"Generated Test Cases as JSON Array:\\n{generated_tests_json}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a62d7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: joblib in /Users/kajalmahata/anaconda3/lib/python3.11/site-packages (1.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8e16547",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   article_id        title                              text   label  \\\n",
      "0         1.1  Article 1.1  This is the text of article 1.1.  case_1   \n",
      "1         1.2  Article 1.2  This is the text of article 1.2.  case_2   \n",
      "2         1.3  Article 1.3  This is the text of article 1.3.  case_3   \n",
      "3         1.4  Article 1.4  This is the text of article 1.4.  case_0   \n",
      "4         1.5  Article 1.5  This is the text of article 1.5.  case_1   \n",
      "..        ...          ...                               ...     ...   \n",
      "95        4.6  Article 4.6  This is the text of article 4.6.  case_0   \n",
      "96        4.7  Article 4.7  This is the text of article 4.7.  case_1   \n",
      "97        5.8  Article 5.8  This is the text of article 5.8.  case_2   \n",
      "98        5.9  Article 5.9  This is the text of article 5.9.  case_3   \n",
      "99        5.0  Article 5.0  This is the text of article 5.0.  case_0   \n",
      "\n",
      "    baselineId  \n",
      "0            1  \n",
      "1            1  \n",
      "2            1  \n",
      "3            1  \n",
      "4            1  \n",
      "..         ...  \n",
      "95           4  \n",
      "96           4  \n",
      "97           5  \n",
      "98           5  \n",
      "99           5  \n",
      "\n",
      "[100 rows x 5 columns]\n",
      "Mean Squared Error on Test Set: 3.0777180989583335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kajalmahata/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found unknown categories ['Article 2.11'] in column 0 during transform",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 97\u001b[0m\n\u001b[1;32m     93\u001b[0m new_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(new_articles)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Predict cases\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Transform article_id and text features\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m new_article_id_encoded \u001b[38;5;241m=\u001b[39m one_hot_encoder\u001b[38;5;241m.\u001b[39mtransform(new_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[1;32m     98\u001b[0m new_text_features \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform(new_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Combine features\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:1016\u001b[0m, in \u001b[0;36mOneHotEncoder.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;66;03m# validation of X happens in _check_X called by _transform\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m warn_on_unknown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_unknown \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[1;32m   1013\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1014\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfrequent_if_exist\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1015\u001b[0m }\n\u001b[0;32m-> 1016\u001b[0m X_int, X_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform(\n\u001b[1;32m   1017\u001b[0m     X,\n\u001b[1;32m   1018\u001b[0m     handle_unknown\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_unknown,\n\u001b[1;32m   1019\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1020\u001b[0m     warn_on_unknown\u001b[38;5;241m=\u001b[39mwarn_on_unknown,\n\u001b[1;32m   1021\u001b[0m )\n\u001b[1;32m   1023\u001b[0m n_samples, n_features \u001b[38;5;241m=\u001b[39m X_int\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_drop_idx_after_grouping \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:199\u001b[0m, in \u001b[0;36m_BaseEncoder._transform\u001b[0;34m(self, X, handle_unknown, force_all_finite, warn_on_unknown, ignore_category_indices)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle_unknown \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    195\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    196\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound unknown categories \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m in column \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m during transform\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(diff, i)\n\u001b[1;32m    198\u001b[0m     )\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m warn_on_unknown:\n",
      "\u001b[0;31mValueError\u001b[0m: Found unknown categories ['Article 2.11'] in column 0 during transform"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Data generation\n",
    "num_baselines = 5\n",
    "min_articles_per_baseline = 20\n",
    "max_articles_per_baseline = 30\n",
    "\n",
    "# Initialize lists to hold the data\n",
    "article_ids = []\n",
    "titles = []\n",
    "texts = []\n",
    "labels = []\n",
    "baseline_ids = []\n",
    "\n",
    "article_counter = 1\n",
    "\n",
    "for baseline_id in range(1, num_baselines + 1):\n",
    "    num_articles = np.random.randint(min_articles_per_baseline, max_articles_per_baseline + 1)\n",
    "    for _ in range(num_articles):\n",
    "        article_id = f\"{baseline_id}.{article_counter % 10}\"  # Using string format\n",
    "        title = f\"Article {article_id}\"\n",
    "        text = f\"This is the text of article {article_id}.\"\n",
    "        label = f\"case_{article_counter % 4}\"  # Alternating between case_0, case_1, case_2, case_3\n",
    "        article_ids.append(article_id)\n",
    "        titles.append(title)\n",
    "        texts.append(text)\n",
    "        labels.append(label)\n",
    "        baseline_ids.append(baseline_id)\n",
    "        article_counter += 1\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"article_id\": article_ids,\n",
    "    \"title\": titles,\n",
    "    \"text\": texts,\n",
    "    \"label\": labels,\n",
    "    \"baselineId\": baseline_ids\n",
    "})\n",
    "print(df.head(100))\n",
    "\n",
    "# Label processing\n",
    "df['num_cases'] = df['label'].apply(lambda x: int(x.split('_')[1]))  # Example: case_0 -> 0\n",
    "\n",
    "# Encoding for article_id and title\n",
    "# Convert article_id to one-hot encoding\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "article_id_encoded = one_hot_encoder.fit_transform(df[['title']])\n",
    "\n",
    "# Feature extraction\n",
    "vectorizer = CountVectorizer()\n",
    "text_features = vectorizer.fit_transform(df['text']).toarray()\n",
    "\n",
    "# Combine features\n",
    "X = np.hstack((article_id_encoded, text_features))\n",
    "y = df['num_cases']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error on Test Set: {mse}\")\n",
    "\n",
    "# Save the model and vectorizer\n",
    "# joblib.dump(model, 'linear_regression_model.pkl')\n",
    "# joblib.dump(vectorizer, 'vectorizer.pkl')\n",
    "# joblib.dump(one_hot_encoder, 'one_hot_encoder.pkl')\n",
    "\n",
    "# Load the model and vectorizer for prediction\n",
    "# model = joblib.load('linear_regression_model.pkl')\n",
    "# vectorizer = joblib.load('vectorizer.pkl')\n",
    "# one_hot_encoder = joblib.load('one_hot_encoder.pkl')\n",
    "\n",
    "# New baseline and articles\n",
    "new_articles = [\n",
    "    {\"article_id\": \"1.1\", \"title\": \"Article 1.1\", \"text\": \"This is the text of article 1.1.\"},\n",
    "    {\"article_id\": \"2.11\", \"title\": \"Article 2.11\", \"text\": \"This is the text of article 2.11.\"},\n",
    "    {\"article_id\": \"2.3\", \"title\": \"Article 2.3\", \"text\": \"This is the text of article 2.3.\"},\n",
    "]\n",
    "\n",
    "new_df = pd.DataFrame(new_articles)\n",
    "\n",
    "# Predict cases\n",
    "# Transform article_id and text features\n",
    "new_article_id_encoded = one_hot_encoder.transform(new_df[['title']])\n",
    "new_text_features = vectorizer.transform(new_df['text']).toarray()\n",
    "\n",
    "# Combine features\n",
    "new_features = np.hstack((new_article_id_encoded, new_text_features))\n",
    "predicted_cases = model.predict(new_features)\n",
    "\n",
    "# Add predictions to DataFrame\n",
    "new_df['predicted_num_cases'] = np.clip(np.round(predicted_cases).astype(int), 0, None)\n",
    "\n",
    "# Display results\n",
    "print(new_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc0a989e",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-66e0df2f-34db956c04c9bba971f416b8;36440f7e-54d5-4e8a-a50e-52a9b598a0af)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Meta-Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mGatedRepoError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/utils/hub.py:402\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[1;32m    403\u001b[0m         path_or_repo_id,\n\u001b[1;32m    404\u001b[0m         filename,\n\u001b[1;32m    405\u001b[0m         subfolder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(subfolder) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m subfolder,\n\u001b[1;32m    406\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[1;32m    407\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m    408\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m    409\u001b[0m         user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[1;32m    410\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m    411\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m    412\u001b[0m         resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[1;32m    413\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m    414\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    415\u001b[0m     )\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:101\u001b[0m, in \u001b[0;36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1240\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_cache_dir(\n\u001b[1;32m   1241\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m   1242\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m   1243\u001b[0m         \u001b[38;5;66;03m# File info\u001b[39;00m\n\u001b[1;32m   1244\u001b[0m         repo_id\u001b[38;5;241m=\u001b[39mrepo_id,\n\u001b[1;32m   1245\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[1;32m   1246\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[1;32m   1247\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m   1248\u001b[0m         \u001b[38;5;66;03m# HTTP info\u001b[39;00m\n\u001b[1;32m   1249\u001b[0m         endpoint\u001b[38;5;241m=\u001b[39mendpoint,\n\u001b[1;32m   1250\u001b[0m         etag_timeout\u001b[38;5;241m=\u001b[39metag_timeout,\n\u001b[1;32m   1251\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m   1252\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m   1253\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m   1254\u001b[0m         \u001b[38;5;66;03m# Additional options\u001b[39;00m\n\u001b[1;32m   1255\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   1256\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m   1257\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1347\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1346\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[0;32m-> 1347\u001b[0m     _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n\u001b[1;32m   1349\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1854\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[1;32m   1853\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1854\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1855\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1856\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1751\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m get_hf_file_metadata(\n\u001b[1;32m   1752\u001b[0m         url\u001b[38;5;241m=\u001b[39murl, proxies\u001b[38;5;241m=\u001b[39mproxies, timeout\u001b[38;5;241m=\u001b[39metag_timeout, headers\u001b[38;5;241m=\u001b[39mheaders, token\u001b[38;5;241m=\u001b[39mtoken\n\u001b[1;32m   1753\u001b[0m     )\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1673\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1672\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1673\u001b[0m r \u001b[38;5;241m=\u001b[39m _request_wrapper(\n\u001b[1;32m   1674\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHEAD\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1675\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m   1676\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m   1677\u001b[0m     allow_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1678\u001b[0m     follow_relative_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1679\u001b[0m     proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m   1680\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m   1681\u001b[0m )\n\u001b[1;32m   1682\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:376\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 376\u001b[0m     response \u001b[38;5;241m=\u001b[39m _request_wrapper(\n\u001b[1;32m    377\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    378\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m    379\u001b[0m         follow_relative_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    381\u001b[0m     )\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:400\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    399\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 400\u001b[0m hf_raise_for_status(response)\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:321\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    318\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot access gated repo for url \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m     )\n\u001b[0;32m--> 321\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m GatedRepoError(message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error_message \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccess to this resource is disabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mGatedRepoError\u001b[0m: 401 Client Error. (Request ID: Root=1-66e0df2f-34db956c04c9bba971f416b8;36440f7e-54d5-4e8a-a50e-52a9b598a0af)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Meta-Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Meta-Llama-3.1-8B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m pipe \u001b[38;5;241m=\u001b[39m pipeline(\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel_id,\n\u001b[1;32m      8\u001b[0m     model_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mbfloat16},\n\u001b[1;32m      9\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     12\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     13\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWho are you? Please, answer in pirate-speak.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     14\u001b[0m ]\n\u001b[1;32m     15\u001b[0m outputs \u001b[38;5;241m=\u001b[39m pipe(\n\u001b[1;32m     16\u001b[0m     messages,\n\u001b[1;32m     17\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m,\n\u001b[1;32m     18\u001b[0m     do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     19\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/pipelines/__init__.py:805\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    802\u001b[0m                 adapter_config \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m    803\u001b[0m                 model \u001b[38;5;241m=\u001b[39m adapter_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase_model_name_or_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 805\u001b[0m     config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    806\u001b[0m         model, _from_pipeline\u001b[38;5;241m=\u001b[39mtask, code_revision\u001b[38;5;241m=\u001b[39mcode_revision, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs\n\u001b[1;32m    807\u001b[0m     )\n\u001b[1;32m    808\u001b[0m     hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39m_commit_hash\n\u001b[1;32m    810\u001b[0m custom_tasks \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py:976\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    973\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    974\u001b[0m code_revision \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 976\u001b[0m config_dict, unused_kwargs \u001b[38;5;241m=\u001b[39m PretrainedConfig\u001b[38;5;241m.\u001b[39mget_config_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    977\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    978\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/configuration_utils.py:632\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 632\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_config_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[1;32m    634\u001b[0m     original_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/configuration_utils.py:689\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    685\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME) \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m gguf_file\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 689\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m cached_file(\n\u001b[1;32m    690\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m    691\u001b[0m         configuration_file,\n\u001b[1;32m    692\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m    693\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m    694\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m    695\u001b[0m         resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[1;32m    696\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    697\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m    698\u001b[0m         user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[1;32m    699\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m    700\u001b[0m         subfolder\u001b[38;5;241m=\u001b[39msubfolder,\n\u001b[1;32m    701\u001b[0m         _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[1;32m    702\u001b[0m     )\n\u001b[1;32m    703\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/utils/hub.py:420\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _raise_exceptions_for_gated_repo:\n\u001b[1;32m    419\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    421\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to have access to it at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    422\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    423\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    426\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    427\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    429\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    430\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-66e0df2f-34db956c04c9bba971f416b8;36440f7e-54d5-4e8a-a50e-52a9b598a0af)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Meta-Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in."
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you? Please, answer in pirate-speak.\"},\n",
    "]\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=False,\n",
    ")\n",
    "assistant_response = outputs[0][\"generated_text\"][-1][\"content\"]\n",
    "print(assistant_response)\n",
    "# Arrrr, me hearty! Yer lookin' fer a bit o' information about meself, eh? Alright then, matey! I be a language-generatin' swashbuckler, a digital buccaneer with a penchant fer spinnin' words into gold doubloons o' knowledge! Me name be... (dramatic pause)...Assistant! Aye, that be me name, and I be here to help ye navigate the seven seas o' questions and find the hidden treasure o' answers! So hoist the sails and set course fer adventure, me hearty! What be yer first question?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
